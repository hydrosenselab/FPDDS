{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data handlers\n",
    "from dask_jobqueue import *\n",
    "from dask.distributed import *\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import netCDF4 \n",
    "from dask import delayed\n",
    "from dask import compute\n",
    "from dask.diagnostics import*\n",
    "from tqdm import tqdm\n",
    "import dask\n",
    "## Plots\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "## PDF generator \n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.pagesizes import A4\n",
    "from reportlab.lib.pagesizes import portrait\n",
    "from reportlab.platypus import Image\n",
    "from reportlab.platypus import Table\n",
    "from reportlab.lib import colors\n",
    "## Global config\n",
    "import os, sys, glob\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import config \n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Request dask workers using PBS for 3 hour##\n",
    "#cluster = PBSCluster(cores=6,memory='64GB',queue='low',project='civil',interface='ib0',walltime='02:00:00')\n",
    "#cluster.scale(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://172.20.9.177:32808</li>\n",
       "  <li><b>Dashboard: </b><a href='http://172.20.9.177:36159/status' target='_blank'>http://172.20.9.177:36159/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>42.66 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://172.20.9.177:32808' processes=2 threads=4, memory=42.66 GB>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Keep refreshing this cell till you see the workers and cores after job has started. ##\n",
    "client = Client(cluster) \n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24471/24471 [00:00<00:00, 2130408.34it/s]\n",
      "100%|██████████| 24471/24471 [00:00<00:00, 2119279.25it/s]\n"
     ]
    }
   ],
   "source": [
    "## Build the list of files in LIS output directory\n",
    "lsm_files = []\n",
    "route_files = []\n",
    "\n",
    "for file in tqdm(sorted(glob.glob('/home/civil/phd/cez198621/msaharia/LDAS/02-ILDAS/OUTPUT/EXP002/SURFACEMODEL/*/*HIST*'))) :\n",
    "    lsm_files.append(file)\n",
    "for file in tqdm(sorted(glob.glob('/home/civil/phd/cez198621/msaharia/LDAS/02-ILDAS/OUTPUT/EXP002/ROUTING/*/*HIST*'))) :\n",
    "    route_files.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of valid stations: 284\n"
     ]
    }
   ],
   "source": [
    " ## Find the list of valid stations having more than 5x365 observations and also find the earliest year of observation.##\n",
    " ## Using Dask Delayed to parallel execution ##\n",
    "\n",
    "def valid_stations (read_dir,min_values,key) :\n",
    "        try :\n",
    "            key_4 =  format(key, \"04\")\n",
    "            gauge_id = 'IWM-gauge-'+str(format(key_4))\n",
    "            station = pd.read_csv(read_dir+gauge_id+'.csv').dropna(subset=['Streamflow (cumecs)'])\n",
    "            value_count = station['Streamflow (cumecs)'].count()\n",
    "            if value_count >= min_values :\n",
    "                return gauge_id\n",
    "        except Exception as e:\n",
    "            return\n",
    "read_dir = ('/home/civil/phd/cez198621/projects/IWM_verification/IMDAA_runs/observed_data/IFI-Observations/flow/')        \n",
    "min_values = 5*365\n",
    "n_stations = 3900\n",
    "dask_results = []\n",
    "for key in range(0,n_stations) :\n",
    "    dask_result = dask.delayed(valid_stations)(read_dir,min_values,key)\n",
    "    dask_results.append(dask_result)\n",
    "dask_compute = dask.compute(*dask_results)\n",
    "valid_stations = [] \n",
    "for val in dask_compute: \n",
    "    if val != None : \n",
    "        valid_stations.append(val) \n",
    "print('Number of valid stations:',str(len(valid_stations)))\n",
    "# Save the valid guage files in a separate directory for later use ##\n",
    "for i in tqdm(range(0,len(valid_stations))):\n",
    "    key = valid_stations[i]\n",
    "    station = pd.read_csv('/home/civil/phd/cez198621/projects/IWM_verification/IMDAA_runs/observed_data/IFI-Observations/flow/'+key+'.csv')\n",
    "    station.set_index('Date',inplace=True)\n",
    "    station.to_csv('/home/civil/phd/cez218606/LISF1/Results/PRINCETON/processed_data/'+key+'-obs.csv')\n",
    "## Save the list of valid gaugeIDs ##\n",
    "valid_df = pd.DataFrame(data={\"stations\": valid_stations})\n",
    "valid_df.to_csv('/home/civil/phd/cez218606/LISF1/Results/PRINCETON/processed_data/valid_stations_list.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using hit and trial, match the LIS file index with observation start date (1959-04-06) and build index for batches  of 2 years##\n",
    "batch_index = [4112,5843,9496,13148,16801,20453,24472]\n",
    "## Open gauge metadata file and set index as GaugeId ##\n",
    "meta_file = pd.read_csv('/home/civil/phd/cez218606/LISF1/Results/PRINCETON/observed_data/gaugemetadata.csv')\n",
    "meta_file.reset_index(drop = True,inplace=True)\n",
    "meta_file.set_index('GaugeID',inplace=True)\n",
    "## Create the list of desired variables to be extracted. SoilMoist will be extracted separately for different profiles. ##\n",
    "lsm_vars = ['Evap_tavg','TotalPrecip_tavg']\n",
    "route_vars = ['FloodedFrac_tavg','RiverDepth_tavg','SWS_tavg','Streamflow_tavg']\n",
    "## Create the empty containers to store gauge-wise extractions.## \n",
    "batch_vars = [None]*len(valid_stations)\n",
    "merged_vars = [None]*len(valid_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Load LIS data in the batches using \"open_mfdataset\" and then load it in RAM using \".compute\" ##\n",
    "for n in tqdm(range (0,(len(batch_index)-1))):\n",
    "    lsmdat = xr.open_mfdataset(lsm_files[batch_index[n]:batch_index[n+1]],combine='by_coords',parallel=True)\n",
    "    routedat=xr.open_mfdataset(route_files[batch_index[n]:batch_index[n+1]],combine='by_coords',parallel = True)\n",
    "    lsmdat = config.reformat_LIS_output(lsmdat)\n",
    "    routedat = config.reformat_LIS_output(routedat)\n",
    "    lsmdat  = lsmdat.chunk({'time':365})\n",
    "    routedat = routedat.chunk({'time':365})\n",
    "    lsmdat = lsmdat.compute()\n",
    "    routedat = routedat.compute()\n",
    "    \n",
    "    ## Iterate over the gauge stations and extract required simulated data ##\n",
    "    for i in tqdm(range(0,len(valid_stations))):\n",
    "        gauge_id = valid_stations[i]\n",
    "        gauge_lat = meta_file.loc[gauge_id,'Latitude']\n",
    "        gauge_lon = meta_file.loc[gauge_id,'Longitude']\n",
    "        lsmdat_sel = lsmdat.sel(lat=gauge_lat,lon=gauge_lon,method='nearest')\n",
    "        routedat_sel = routedat.sel(lat=gauge_lat,lon=gauge_lon,method='nearest')\n",
    "        ext_lsm = lsmdat_sel[lsm_vars].to_dataframe()\n",
    "        ext_soil_1 = lsmdat_sel['SoilMoist_tavg'].sel(SoilMoist_profiles=0).to_dataframe()\n",
    "        ext_soil_1 = ext_soil_1.rename(columns={'SoilMoist_tavg': \"SM_L1\"})\n",
    "        ext_soil_2 = lsmdat_sel['SoilMoist_tavg'].sel(SoilMoist_profiles=1).to_dataframe()\n",
    "        ext_soil_2 = ext_soil_2.rename(columns={'SoilMoist_tavg': \"SM_L2\"})\n",
    "        ext_soil_3 = lsmdat_sel['SoilMoist_tavg'].sel(SoilMoist_profiles=2).to_dataframe()\n",
    "        ext_soil_3 = ext_soil_3.rename(columns={'SoilMoist_tavg': \"SM_L3\"})\n",
    "        ext_soil_4 = lsmdat_sel['SoilMoist_tavg'].sel(SoilMoist_profiles=3).to_dataframe()\n",
    "        ext_soil_4 = ext_soil_4.rename(columns={'SoilMoist_tavg': \"SM_L4\"})\n",
    "        ext_route = routedat_sel[route_vars].to_dataframe()\n",
    "        batch_vars[i] = pd.concat([ext_lsm,ext_soil_1,ext_soil_2,ext_soil_3,ext_soil_4,ext_route],axis=1)\n",
    "        batch_vars[i] = batch_vars[i].loc[:,~batch_vars[i].columns.duplicated()]\n",
    "        batch_vars[i] = batch_vars[i].drop(['lat','lon'], axis=1)\n",
    "        merged_vars[i] = pd.concat([merged_vars[i],batch_vars[i]])\n",
    "        \n",
    "    ## Purge the variables to free up RAM ##\n",
    "    del lsmdat\n",
    "    del routedat\n",
    "    del lsmdat_sel\n",
    "    del routedat_sel\n",
    "    gc.collect()\n",
    "    \n",
    "## Save the extracted variables in gauge-wise CSVs ##\n",
    "for i in tqdm(range(0,len(valid_stations))):\n",
    "    key = valid_stations[i]\n",
    "    merged_vars[i].to_csv('/home/civil/phd/cez198621/projects/IWM_verification/IMDAA_runs/processed_data/'+key+'-sim.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For GRDC Data\n",
    "#grdc_filelist =sorted(glob.glob('./../data/input/grdc_monthly/GRDC_csv/*monthly*'))\n",
    "## Using hit and trial, match the LIS file index with observation start date (1959-04-06) and build index for batches  of 10 years##\n",
    "#batch_index = [0,4112,5843,9496,11687]\n",
    "## Open gauge metadata file and set index as GaugeId ##\n",
    "#meta_file = pd.read_csv('../data/input/grdc_monthly/GRDC_csv/metafile.csv')\n",
    "# meta_file.reset_index(drop = True,inplace=True)\n",
    "# meta_file.set_index('GRDC_No',inplace=True)\n",
    "## Create the list of desired variables to be extracted. SoilMoist will be extracted separately for different profiles. ##\n",
    "#lsm_vars = ['Evap_tavg','TotalPrecip_tavg']\n",
    "#route_vars = ['FloodedFrac_tavg','RiverDepth_tavg','SWS_tavg','Streamflow_tavg']\n",
    "## Create the empty containers to store gauge-wise extractions.## \n",
    "#batch_vars = [None]*len(grdc_filelist)\n",
    "#merged_vars = [None]*len(grdc_filelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "  0%|          | 0/44 [00:00<?, ?it/s]\u001b[A\n",
      " 11%|█▏        | 5/44 [00:00<00:00, 47.40it/s]\u001b[A\n",
      " 25%|██▌       | 11/44 [00:00<00:00, 50.35it/s]\u001b[A\n",
      " 36%|███▋      | 16/44 [00:00<00:00, 48.92it/s]\u001b[A\n",
      " 50%|█████     | 22/44 [00:00<00:00, 51.22it/s]\u001b[A\n",
      " 64%|██████▎   | 28/44 [00:00<00:00, 52.96it/s]\u001b[A\n",
      " 77%|███████▋  | 34/44 [00:00<00:00, 54.71it/s]\u001b[A\n",
      "100%|██████████| 44/44 [00:00<00:00, 55.49it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [17:13<51:39, 1033.23s/it]\n",
      "  0%|          | 0/44 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|█▎        | 6/44 [00:00<00:00, 54.69it/s]\u001b[A\n",
      " 27%|██▋       | 12/44 [00:00<00:00, 55.12it/s]\u001b[A\n",
      " 41%|████      | 18/44 [00:00<00:00, 55.92it/s]\u001b[A\n",
      " 55%|█████▍    | 24/44 [00:00<00:00, 56.80it/s]\u001b[A\n",
      " 68%|██████▊   | 30/44 [00:00<00:00, 57.16it/s]\u001b[A\n",
      " 82%|████████▏ | 36/44 [00:00<00:00, 57.49it/s]\u001b[A\n",
      "100%|██████████| 44/44 [00:00<00:00, 57.43it/s]\u001b[A\n",
      " 50%|█████     | 2/4 [23:39<27:58, 839.22s/it] \n",
      "  0%|          | 0/44 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/44 [00:00<00:08,  5.14it/s]\u001b[A\n",
      " 16%|█▌        | 7/44 [00:00<00:05,  7.05it/s]\u001b[A\n",
      " 30%|██▉       | 13/44 [00:00<00:03,  9.56it/s]\u001b[A\n",
      " 43%|████▎     | 19/44 [00:00<00:01, 12.76it/s]\u001b[A\n",
      " 57%|█████▋    | 25/44 [00:00<00:01, 16.60it/s]\u001b[A\n",
      " 70%|███████   | 31/44 [00:00<00:00, 21.06it/s]\u001b[A\n",
      " 84%|████████▍ | 37/44 [00:00<00:00, 25.96it/s]\u001b[A\n",
      "100%|██████████| 44/44 [00:00<00:00, 45.84it/s]\u001b[A\n",
      " 75%|███████▌  | 3/4 [39:04<14:24, 864.91s/it]\n",
      "  0%|          | 0/44 [00:00<?, ?it/s]\u001b[A\n",
      "  9%|▉         | 4/44 [00:00<00:01, 36.15it/s]\u001b[A\n",
      " 23%|██▎       | 10/44 [00:00<00:00, 40.87it/s]\u001b[A\n",
      " 36%|███▋      | 16/44 [00:00<00:00, 44.65it/s]\u001b[A\n",
      " 50%|█████     | 22/44 [00:00<00:00, 47.73it/s]\u001b[A\n",
      " 64%|██████▎   | 28/44 [00:00<00:00, 50.74it/s]\u001b[A\n",
      " 77%|███████▋  | 34/44 [00:00<00:00, 52.96it/s]\u001b[A\n",
      "100%|██████████| 44/44 [00:00<00:00, 55.00it/s]\u001b[A\n",
      "100%|██████████| 4/4 [47:53<00:00, 718.48s/it]\n",
      "100%|██████████| 44/44 [00:00<00:00, 81.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29min 14s, sys: 1min 15s, total: 30min 29s\n",
      "Wall time: 47min 54s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "# For GRDC Data\n",
    "## Load LIS data in the batches using \"open_mfdataset\" and then load it in RAM using \".compute\" ##\n",
    "#for n in tqdm(range (0,(len(batch_index)-1))):\n",
    "#    lsmdat = xr.open_mfdataset(lsm_files[batch_index[n]:batch_index[n+1]],combine='by_coords',parallel=True)\n",
    "#    routedat=xr.open_mfdataset(route_files[batch_index[n]:batch_index[n+1]],combine='by_coords',parallel = True)\n",
    "#    lsmdat = config.reformat_LIS_output(lsmdat)\n",
    "#    routedat = config.reformat_LIS_output(routedat)\n",
    "#    lsmdat  = lsmdat.chunk({'time':365})\n",
    "#    routedat = routedat.chunk({'time':365})\n",
    "#    lsm_monthly =lsmdat.resample(time=\"1M\").mean()\n",
    "#    route_monthly = routedat.resample(time=\"1M\").mean()\n",
    "#    lsmdat = lsm_monthly.compute()\n",
    "#    routedat = route_monthly.compute()\n",
    "#    \n",
    "#    ## Iterate over the gauge stations and extract required simulated data ##\n",
    "#    for i in tqdm(range(0,len(grdc_filelist))):\n",
    "#        gauge_id = meta_file.loc[i,'GRDC_No']\n",
    "#        gauge_lat = meta_file.loc[i,'Latitude']\n",
    "#        gauge_lon = meta_file.loc[i,'Longitude']\n",
    "#        lsmdat_sel = lsmdat.sel(lat=gauge_lat,lon=gauge_lon,method='nearest')\n",
    "#        routedat_sel = routedat.sel(lat=gauge_lat,lon=gauge_lon,method='nearest')\n",
    "#        ext_lsm = lsmdat_sel[lsm_vars].to_dataframe()\n",
    "#        ext_soil_1 = lsmdat_sel['SoilMoist_tavg'].sel(SoilMoist_profiles=0).to_dataframe()\n",
    "#        ext_soil_1 = ext_soil_1.rename(columns={'SoilMoist_tavg': \"SM_L1\"})\n",
    " #       ext_soil_2 = lsmdat_sel['SoilMoist_tavg'].sel(SoilMoist_profiles=1).to_dataframe()\n",
    " #       ext_soil_2 = ext_soil_2.rename(columns={'SoilMoist_tavg': \"SM_L2\"})\n",
    " #       ext_soil_3 = lsmdat_sel['SoilMoist_tavg'].sel(SoilMoist_profiles=2).to_dataframe()\n",
    " #       ext_soil_3 = ext_soil_3.rename(columns={'SoilMoist_tavg': \"SM_L3\"})\n",
    " #       ext_soil_4 = lsmdat_sel['SoilMoist_tavg'].sel(SoilMoist_profiles=3).to_dataframe()\n",
    " #       ext_soil_4 = ext_soil_4.rename(columns={'SoilMoist_tavg': \"SM_L4\"})\n",
    " #       ext_route = routedat_sel[route_vars].to_dataframe()\n",
    " #       batch_vars[i] = pd.concat([ext_lsm,ext_soil_1,ext_soil_2,ext_soil_3,ext_soil_4,ext_route],axis=1)\n",
    " #       batch_vars[i] = batch_vars[i].loc[:,~batch_vars[i].columns.duplicated()]\n",
    " #       batch_vars[i] = batch_vars[i].drop(['lat','lon'], axis=1)\n",
    " #       merged_vars[i] = pd.concat([merged_vars[i],batch_vars[i]])\n",
    " #       \n",
    "  #  ## Purge the variables to free up RAM ##\n",
    " #   del lsmdat\n",
    " #   del routedat\n",
    " #   del lsmdat_sel\n",
    " #   del routedat_sel\n",
    " #   gc.collect()\n",
    " #   \n",
    "## Save the extracted variables in gauge-wise CSVs ##\n",
    "#for i in tqdm(range(0,len(grdc_filelist))):\n",
    " #   key = meta_file.loc[i,'GRDC_No']\n",
    "#    merged_vars[i].to_csv('../data/output/grdc_sim/'+str(key)+'_sim.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [00:02<00:00, 18.57it/s]\n"
     ]
    }
   ],
   "source": [
    "## Append LIS extractions to observed data and save as new CSVs####\n",
    "#meta_file = pd.read_csv('../data/input/grdc_monthly/GRDC_csv/metafile.csv')\n",
    "#grdc_filelist =sorted(glob.glob('./../data/input/grdc_monthly/GRDC_csv/*monthly*'))\n",
    "#for i in tqdm(range (0,len(grdc_filelist))):\n",
    "#    key = meta_file.loc[i,'GRDC_No']\n",
    "#    obs = pd.read_csv('../data/input/grdc_monthly/GRDC_csv/'+str(key)+'_monthly.csv')\n",
    "#    sim = pd.read_csv('../data/output/grdc_sim/'+str(key)+'_sim.csv')\n",
    "#    sim = sim.rename(columns={\"time\": \"Date\"})\n",
    "#    obs = obs.join(sim.set_index('Date'), on='Date')\n",
    "#    obs.to_csv('../data/output/grdc_sim/grdc_merged/'+str(key)+'_merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
