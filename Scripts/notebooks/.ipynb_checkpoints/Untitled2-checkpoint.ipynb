{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3242b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python modules load Complete\n",
      "Python Run Complete\n"
     ]
    }
   ],
   "source": [
    "## Data handlers\n",
    "from dask_jobqueue import *\n",
    "from dask.distributed import *\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import netCDF4 \n",
    "from dask import delayed\n",
    "from dask import compute\n",
    "from dask.diagnostics import*\n",
    "from tqdm import tqdm\n",
    "import dask\n",
    "\n",
    "## Global config\n",
    "import os, sys, glob\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import config \n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Python modules load Complete\")\n",
    "\n",
    "## Build the list of files in LIS output directory\n",
    "route_files = []\n",
    "for file in sorted(glob.glob('/home/civil/phd/cez218606/LISF1/PDDS/output/LSM_MERRA2_1/ROUTING/*/*HIST*')) :\n",
    "    route_files.append(file)\n",
    "\n",
    " ## Find the list of valid stations having more than 5x365 observations and also find the earliest year of observation.##\n",
    " ## Using Dask Delayed to parallel execution ##\n",
    "\n",
    "def valid_stations (read_dir,min_values,key) :\n",
    "        try :\n",
    "            key_4 =  format(key, \"03\")\n",
    "            gauge_id = 'IWM-gauge-'+str(format(key_4))\n",
    "            station = pd.read_csv(read_dir+gauge_id+'.csv').dropna(subset=['Streamflow (cumecs)'])\n",
    "            value_count = station['Streamflow (cumecs)'].count()\n",
    "            if value_count >= min_values :\n",
    "                return gauge_id\n",
    "        except Exception as e:\n",
    "            return\n",
    "read_dir = ('/home/civil/phd/cez218606/LISF1/PDDS/output/Narmada/')        \n",
    "min_values = 5*365\n",
    "n_stations = 3900\n",
    "dask_results = []\n",
    "for key in range(0,n_stations) :\n",
    "    dask_result = dask.delayed(valid_stations)(read_dir,min_values,key)\n",
    "    dask_results.append(dask_result)\n",
    "dask_compute = dask.compute(*dask_results)\n",
    "valid_stations = [] \n",
    "for val in dask_compute: \n",
    "    if val != None : \n",
    "        valid_stations.append(val) \n",
    "# Save the valid guage files in a separate directory for later use ##\n",
    "for i in range(0,len(valid_stations)):\n",
    "    key = valid_stations[i]\n",
    "    station = pd.read_csv('/home/civil/phd/cez218606/LISF1/PDDS/output/Narmada/'+key+'.csv')\n",
    "    station.set_index('Date',inplace=True)\n",
    "    station.to_csv('/home/civil/phd/cez218606/LISF1/PDDS/output/processed_data1/'+key+'-obs.csv')\n",
    "## Save the list of valid gaugeIDs ##\n",
    "valid_df = pd.DataFrame(data={\"stations\": valid_stations})\n",
    "valid_df.to_csv('/home/civil/phd/cez218606/LISF1/PDDS/output/processed_data1/valid_stations_list.csv',index=False)\n",
    "\n",
    "## Using hit and trial, match the LIS file index with observation start date (1959-04-06) and build index for batches  of 2 years##\n",
    "#batch_index = [12053,12755,13512,14242,14973,15703,16434,17164,17894,18624,19354,20084,20814,21053,22274,23004,23734,24470]\n",
    "batch_index = [0,364,730,1095]\n",
    "## Open gauge metadata file and set index as GaugeId ##\n",
    "meta_file = pd.read_csv(\"/home/civil/phd/cez218606/LISF1/PDDS/output/Narmada.csv\")\n",
    "meta_file.reset_index(drop = True,inplace=True)\n",
    "meta_file.set_index('GaugeID',inplace=True)\n",
    "## Create the list of desired variables to be extracted. SoilMoist will be extracted separately for different profiles. ##\n",
    "route_vars = ['Streamflow_tavg']\n",
    "## Create the empty containers to store gauge-wise extractions.## \n",
    "batch_vars = [None]*len(valid_stations)\n",
    "merged_vars = [None]*len(valid_stations)\n",
    "\n",
    "#%%time\n",
    "## Load LIS data in the batches using \"open_mfdataset\" and then load it in RAM using \".compute\" ##\n",
    "\n",
    "routedat=xr.open_mfdataset(route_files,combine='by_coords',parallel = True)\n",
    "routedat = config.reformat_LIS_output(routedat)\n",
    "#routedat = routedat.chunk({'time':365})\n",
    "routedat = routedat.compute()\n",
    "    \n",
    "## Iterate over the gauge stations and extract required simulated data ##\n",
    "for i in range(0,len(valid_stations)):\n",
    "    gauge_id = valid_stations[i]\n",
    "    gauge_lat = meta_file.loc[gauge_id,'Latitude']\n",
    "    gauge_lon = meta_file.loc[gauge_id,'Longitude']\n",
    "    routedat_sel = routedat.sel(lat=gauge_lat,lon=gauge_lon,method='nearest')\n",
    "    ext_route = routedat_sel[route_vars].to_dataframe()\n",
    "        \n",
    "    batch_vars[i] = pd.concat([ext_route],axis=1)\n",
    "    batch_vars[i] = batch_vars[i].loc[:,~batch_vars[i].columns.duplicated()]\n",
    "    batch_vars[i] = batch_vars[i].drop(['lat','lon'], axis=1)\n",
    "    merged_vars[i] = pd.concat([merged_vars[i],batch_vars[i]])\n",
    "        \n",
    "## Purge the variables to free up RAM ##\n",
    "del routedat\n",
    "del routedat_sel\n",
    "gc.collect()    \n",
    "## Save the extracted variables in gauge-wise CSVs ##\n",
    "for i in range(0,len(valid_stations)):\n",
    "    key = valid_stations[i]\n",
    "    merged_vars[i].to_csv('/home/civil/phd/cez218606/LISF1/PDDS/output/processed_data1/'+key+'-sim.csv')\n",
    "\n",
    "    \n",
    "del merged_vars\n",
    "\n",
    "## Append LIS extractions to observed data and save as new CSVs####\n",
    "for i in range (0,len(valid_stations)):\n",
    "    key = valid_stations[i]\n",
    "    obs = pd.read_csv('/home/civil/phd/cez218606/LISF1/PDDS/output/processed_data1/'+key+'-obs.csv')\n",
    "    obs.set_index('Date',inplace=True)\n",
    "    obs.index = pd.to_datetime(obs.index)\n",
    "    sim = pd.read_csv('/home/civil/phd/cez218606/LISF1/PDDS/output/processed_data1/'+key+'-sim.csv')\n",
    "    sim.set_index('time',inplace=True)\n",
    "    sim.index = pd.to_datetime(sim.index)\n",
    "    obs = pd.concat([obs,sim],axis =1).reindex(obs.index)\n",
    "    obs.to_csv('/home/civil/phd/cez218606/LISF1/PDDS/output/processed_data1/'+key+'-merged.csv')\n",
    "\n",
    "def covariance(x, y):\n",
    "    return np.dot(x - x.mean(), y - y.mean()) / x.count()\n",
    "\n",
    "def corrrelation(x, y):\n",
    "    return covariance(x, y) / (x.std() * y.std())\n",
    "\n",
    "def R2(x, y):\n",
    "    return (corrrelation(x, y))**2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load pre-requisite data\n",
    "stations_list = pd.read_csv('/home/civil/phd/cez218606/LISF1/PDDS/output/processed_data1/valid_stations_list.csv')\n",
    "meta_file = pd.read_csv('/home/civil/phd/cez218606/LISF1/PDDS/output/Narmada.csv')\n",
    "meta_file.reset_index(drop = True,inplace=True)\n",
    "meta_file.set_index('GaugeID',inplace=True)\n",
    "obj_fn = [config.pbias,config.nashsutcliffe,config.kge,\n",
    "          config.rmse,config.correlationcoefficient,config.mae,config.rrmse]\n",
    "obj_fn_names = ['PBIAS','NSE','KGE','RMSE','CORR','MAE','MAPE']\n",
    "error_columns = ['GaugeID','Latitude','Longitude','PBIAS','NSE','KGE','RMSE','CORR','MAE','RRMSE','obs_avg','sim_avg','MAPE','R2']\n",
    "error_df = pd.DataFrame(columns = error_columns).set_index('GaugeID')\n",
    "\n",
    "key = stations_list.loc[0,'stations']\n",
    "\n",
    "\n",
    "\n",
    "#Create list of all gauge stations with NaNs dropped and export to CSVs(if needed).\n",
    "stations = []\n",
    "for i in range(0,len(stations_list)):\n",
    "    key = stations_list.loc[i,'stations']\n",
    "    station = pd.read_csv('/home/civil/phd/cez218606/LISF1/PDDS/output/processed_data1/'+key+'-merged.csv')\n",
    "    station.set_index('Date',inplace=True)\n",
    "    station.index = pd.to_datetime(station.index)\n",
    "    #station.dropna(inplace=True)\n",
    "    station = station.loc['2001-01-01':'2002-12-30']\n",
    "    stations.append(station)\n",
    "\n",
    "new_row = [None]*len(stations)\n",
    "for i in range(0,len(stations)):\n",
    "    try:\n",
    "        obs_flow = stations[i]['Streamflow (cumecs)']\n",
    "        sim_flow = stations[i]['Streamflow_tavg']\n",
    "        meta_key = stations_list.loc[i,'stations']\n",
    "        s_info= {'lat':meta_file.at[meta_key,'Latitude'],'lon':meta_file.at[meta_key,'Longitude']}\n",
    "        error_columns = ['GaugeID','Latitude','Longitude','PBIAS','NSE','KGE','CORR','Alpha','Beta','RMSE','MAE','RRMSE','obs_avg','sim_avg','NMAE','R2','r_var']\n",
    "        error_df = pd.DataFrame(columns = error_columns).set_index('GaugeID')\n",
    "        new_row[i]= pd.DataFrame ([[meta_key,s_info['lat'],s_info['lon'],\n",
    "                                    round(config.pbias(obs_flow,sim_flow),3),\n",
    "                                    round(config.nashsutcliffe(obs_flow,sim_flow),3),\n",
    "                                    round(config.kge(obs_flow,sim_flow,return_all=\"True\")[0],3),\n",
    "                                    round(config.kge(obs_flow,sim_flow,return_all=\"True\")[1],3),\n",
    "                                    round(config.kge(obs_flow,sim_flow,return_all=\"True\")[2],3),\n",
    "                                    round(config.kge(obs_flow,sim_flow,return_all=\"True\")[3],3),\n",
    "                                    round(config.rmse(obs_flow,sim_flow),3),\n",
    "                                    round(config.mae(obs_flow,sim_flow),3),\n",
    "                                    round(config.rrmse(obs_flow,sim_flow),3),\n",
    "                                    round(obs_flow.mean(),3),\n",
    "                                    round(sim_flow.mean(),3),\n",
    "                                    round(config.nmae(obs_flow,sim_flow),3),\n",
    "                                    R2(obs_flow,sim_flow),\n",
    "                                    (1/((sim_flow-obs_flow).std())**2)]],\n",
    "                                    columns=(['GaugeID','Latitude','Longitude','PBIAS','NSE','KGE','CORR','Alpha','Beta','RMSE','MAE','RRMSE','obs_avg','sim_avg','NMAE','R2','r_var'])).set_index('GaugeID')\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        continue\n",
    "\n",
    "for i in range(0,len(new_row)):\n",
    "    error_df = error_df.append(new_row[i]) \n",
    "error_df['wf'] = (error_df['r_var'])/(error_df['r_var'].sum())\n",
    "error_df.to_csv('/home/civil/phd/cez218606/LISF1/PDDS/output/error_metrix/error_matrix1.csv')\n",
    "print(\"Python Run Complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
