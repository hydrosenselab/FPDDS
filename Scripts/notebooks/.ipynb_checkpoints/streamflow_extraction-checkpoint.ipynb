{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ee71dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/civil/phd/cez218606/anaconda3/envs/geo_env/lib/python3.10/site-packages/dask_jobqueue/core.py:20: FutureWarning: tmpfile is deprecated and will be removed in a future release. Please use dask.utils.tmpfile instead.\n",
      "  from distributed.utils import tmpfile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python modules load Complete\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "## Data handlers\n",
    "from dask_jobqueue import *\n",
    "from dask.distributed import *\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import netCDF4 \n",
    "from dask import delayed\n",
    "from dask import compute\n",
    "from dask.diagnostics import*\n",
    "from tqdm import tqdm\n",
    "import dask\n",
    "\n",
    "## Global config\n",
    "import os, sys, glob\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import config \n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Python modules load Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "480f20aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the list of files in LIS output directory\n",
    "route_files = []\n",
    "for file in sorted(glob.glob('/home/civil/phd/cez228142/Test_Merra/output/imd/LIS_run/ROUTING/*/*HIST*')) :\n",
    "    route_files.append(file)\n",
    "\n",
    " ## Find the list of valid stations having more than 5x365 observations and also find the earliest year of observation.##\n",
    " ## Using Dask Delayed to parallel execution ##\n",
    "\n",
    "def valid_stations (read_dir,min_values,key) :\n",
    "        try :\n",
    "            key_4 =  format(key, \"03\")\n",
    "            gauge_id = 'IWM-gauge-'+str(format(key_4))\n",
    "            station = pd.read_csv(read_dir+gauge_id+'.csv').dropna(subset=['Streamflow (cumecs)'])\n",
    "            value_count = station['Streamflow (cumecs)'].count()\n",
    "            if value_count >= min_values :\n",
    "                return gauge_id\n",
    "        except Exception as e:\n",
    "            return\n",
    "read_dir = ('/scratch/civil/phd/cez218606/CLSM_Calibration/Narmada/output/Narmada/')        \n",
    "min_values = 5*365\n",
    "n_stations = 3900\n",
    "dask_results = []\n",
    "for key in range(0,n_stations) :\n",
    "    dask_result = dask.delayed(valid_stations)(read_dir,min_values,key)\n",
    "    dask_results.append(dask_result)\n",
    "dask_compute = dask.compute(*dask_results)\n",
    "valid_stations = [] \n",
    "for val in dask_compute: \n",
    "    if val != None : \n",
    "        valid_stations.append(val) \n",
    "# Save the valid guage files in a separate directory for later use ##\n",
    "for i in range(0,len(valid_stations)):\n",
    "    key = valid_stations[i]\n",
    "    station = pd.read_csv('/scratch/civil/phd/cez218606/CLSM_Calibration/Narmada/output/Narmada/'+key+'.csv')\n",
    "    station.set_index('Date',inplace=True)\n",
    "    station.to_csv('/home/civil/phd/cez218606/scratch/CLSM_Calibration/Narmada/final_run/output/uncal/'+key+'-obs.csv')\n",
    "## Save the list of valid gaugeIDs ##\n",
    "valid_df = pd.DataFrame(data={\"stations\": valid_stations})\n",
    "valid_df.to_csv('/home/civil/phd/cez218606/scratch/CLSM_Calibration/Narmada/final_run/output/uncal/valid_stations_list.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9c3d87c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14607"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(route_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a39e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using hit and trial, match the LIS file index with observation start date (1959-04-06) and build index for batches  of 2 years##\n",
    "#batch_index = [12053,12755,13512,14242,14973,15703,16434,17164,17894,18624,19354,20084,20814,21053,22274,23004,23734,24470]\n",
    "batch_index = [0,702,1459,2189,2920,3650,4382,5111,5841,6571,7304,8100,9000,9900,10800,11600,12053,12755,13512,14242,14607]\n",
    "## Open gauge metadata file and set index as GaugeId ##\n",
    "meta_file = pd.read_csv(\"/scratch/civil/phd/cez218606/CLSM_Calibration/Narmada/output/Narmada.csv\")\n",
    "meta_file.reset_index(drop = True,inplace=True)\n",
    "meta_file.set_index('GaugeID',inplace=True)\n",
    "## Create the list of desired variables to be extracted. SoilMoist will be extracted separately for different profiles. ##\n",
    "route_vars = ['Streamflow_tavg']\n",
    "## Create the empty containers to store gauge-wise extractions.## \n",
    "batch_vars = [None]*len(valid_stations)\n",
    "merged_vars = [None]*len(valid_stations)\n",
    "\n",
    "#%%time\n",
    "## Load LIS data in the batches using \"open_mfdataset\" and then load it in RAM using \".compute\" ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a90ee4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]\n",
      "100%|██████████████████████████████████████████| 12/12 [00:00<00:00, 311.03it/s]\u001b[A\n",
      "  5%|██▏                                         | 1/20 [00:52<16:38, 52.55s/it]\n",
      "100%|██████████████████████████████████████████| 12/12 [00:00<00:00, 299.16it/s]\u001b[A\n",
      " 10%|████▍                                       | 2/20 [01:49<16:35, 55.30s/it]\n",
      "100%|██████████████████████████████████████████| 12/12 [00:00<00:00, 305.28it/s]\u001b[A\n",
      " 15%|██████▌                                     | 3/20 [02:44<15:39, 55.24s/it]\n",
      "100%|██████████████████████████████████████████| 12/12 [00:00<00:00, 274.43it/s]\u001b[A\n",
      " 20%|████████▊                                   | 4/20 [03:39<14:40, 55.03s/it]\n",
      "100%|██████████████████████████████████████████| 12/12 [00:00<00:00, 283.48it/s]\u001b[A\n",
      " 25%|███████████                                 | 5/20 [04:34<13:46, 55.07s/it]\n",
      "  0%|                                                    | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████| 12/12 [00:00<00:00, 107.86it/s]\u001b[A\n",
      " 30%|█████████████▏                              | 6/20 [05:33<13:08, 56.29s/it]\n",
      "100%|██████████████████████████████████████████| 12/12 [00:00<00:00, 178.29it/s]\u001b[A\n",
      " 35%|███████████████▍                            | 7/20 [06:31<12:18, 56.77s/it]\n",
      "  0%|                                                    | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 88.33it/s]\u001b[A\n",
      " 40%|█████████████████▌                          | 8/20 [07:26<11:14, 56.24s/it]\n",
      "100%|██████████████████████████████████████████| 12/12 [00:00<00:00, 281.40it/s]\u001b[A\n",
      " 45%|███████████████████▊                        | 9/20 [08:21<10:14, 55.85s/it]\n",
      "100%|██████████████████████████████████████████| 12/12 [00:00<00:00, 303.79it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 10/20 [09:15<09:14, 55.43s/it]\n",
      "  0%|                                                    | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 97.62it/s]\u001b[A\n",
      " 55%|███████████████████████▋                   | 11/20 [10:17<08:37, 57.49s/it]\n",
      "  0%|                                                    | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 34.31it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 12/20 [11:25<08:05, 60.63s/it]\n",
      "100%|██████████████████████████████████████████| 12/12 [00:00<00:00, 285.74it/s]\u001b[A\n",
      " 65%|███████████████████████████▉               | 13/20 [12:32<07:17, 62.54s/it]\n",
      "100%|██████████████████████████████████████████| 12/12 [00:00<00:00, 253.19it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 14/20 [13:41<06:26, 64.49s/it]\n",
      "100%|██████████████████████████████████████████| 12/12 [00:00<00:00, 276.55it/s]\u001b[A\n",
      " 75%|████████████████████████████████▎          | 15/20 [14:41<05:15, 63.06s/it]\n",
      "100%|██████████████████████████████████████████| 12/12 [00:00<00:00, 298.36it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 16/20 [15:14<03:35, 53.99s/it]\n",
      "100%|██████████████████████████████████████████| 12/12 [00:00<00:00, 281.18it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▌      | 17/20 [16:07<02:40, 53.62s/it]\n",
      "  0%|                                                    | 0/12 [00:00<?, ?it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 12/12 [00:00<00:00, 84.53it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 18/20 [17:06<01:50, 55.25s/it]\n",
      "100%|██████████████████████████████████████████| 12/12 [00:00<00:00, 293.61it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▊  | 19/20 [18:00<00:55, 55.12s/it]\n",
      "100%|██████████████████████████████████████████| 12/12 [00:00<00:00, 301.74it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 20/20 [18:27<00:00, 55.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 36s, sys: 3min 7s, total: 22min 44s\n",
      "Wall time: 18min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Load LIS data in the batches using \"open_mfdataset\" and then load it in RAM using \".compute\" ##\n",
    "for n in tqdm(range (0,(len(batch_index)-1))):\n",
    "    routedat=xr.open_mfdataset(route_files[batch_index[n]:batch_index[n+1]],combine='by_coords',parallel = True)\n",
    "    routedat = config.reformat_LIS_output(routedat)\n",
    "    #routedat = routedat.chunk({'time':365})\n",
    "    routedat = routedat.compute()\n",
    "    \n",
    "    ## Iterate over the gauge stations and extract required simulated data ##\n",
    "    for i in tqdm(range(0,len(valid_stations))):\n",
    "        gauge_id = valid_stations[i]\n",
    "        gauge_lat = meta_file.loc[gauge_id,'Latitude']\n",
    "        gauge_lon = meta_file.loc[gauge_id,'Longitude']\n",
    "        routedat_sel = routedat.sel(lat=gauge_lat,lon=gauge_lon,method='nearest')\n",
    "        ext_route = routedat_sel[route_vars].to_dataframe()\n",
    "        \n",
    "        batch_vars[i] = pd.concat([ext_route],axis=1)\n",
    "        batch_vars[i] = batch_vars[i].loc[:,~batch_vars[i].columns.duplicated()]\n",
    "        batch_vars[i] = batch_vars[i].drop(['lat','lon'], axis=1)\n",
    "        merged_vars[i] = pd.concat([merged_vars[i],batch_vars[i]])\n",
    "        \n",
    "    ## Purge the variables to free up RAM ##\n",
    "    del routedat\n",
    "    del routedat_sel\n",
    "    gc.collect() \n",
    "## Save the extracted variables in gauge-wise CSVs ##\n",
    "for i in range(0,len(valid_stations)):\n",
    "    key = valid_stations[i]\n",
    "    merged_vars[i].to_csv('/home/civil/phd/cez218606/scratch/CLSM_Calibration/Narmada/final_run/output/uncal/'+key+'-sim.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9764ff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Append LIS extractions to observed data and save as new CSVs####\n",
    "for i in range (0,len(valid_stations)):\n",
    "    key = valid_stations[i]\n",
    "    obs = pd.read_csv('/home/civil/phd/cez218606/scratch/CLSM_Calibration/Narmada/final_run/output/uncal/'+key+'-obs.csv')\n",
    "    obs.set_index('Date',inplace=True)\n",
    "    obs.index = pd.to_datetime(obs.index)\n",
    "    sim = pd.read_csv('/home/civil/phd/cez218606/scratch/CLSM_Calibration/Narmada/final_run/output/uncal/'+key+'-sim.csv')\n",
    "    sim.set_index('time',inplace=True)\n",
    "    sim.index = pd.to_datetime(sim.index)\n",
    "    obs = pd.concat([obs,sim],axis =1).reindex(obs.index)\n",
    "    obs.to_csv('/home/civil/phd/cez218606/scratch/CLSM_Calibration/Narmada/final_run/output/uncal/'+key+'-merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfecb662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance(x, y):\n",
    "    return np.dot(x - x.mean(), y - y.mean()) / x.count()\n",
    "\n",
    "def corrrelation(x, y):\n",
    "    return covariance(x, y) / (x.std() * y.std())\n",
    "\n",
    "def R2(x, y):\n",
    "    return (corrrelation(x, y))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2312ccce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Run Complete\n"
     ]
    }
   ],
   "source": [
    "# Load pre-requisite data\n",
    "stations_list = pd.read_csv('/home/civil/phd/cez218606/scratch/CLSM_Calibration/Narmada/final_run/output/Processed/valid_stations_list.csv')\n",
    "meta_file = pd.read_csv('/scratch/civil/phd/cez218606/CLSM_Calibration/Narmada/output/Narmada.csv')\n",
    "meta_file.reset_index(drop = True,inplace=True)\n",
    "meta_file.set_index('GaugeID',inplace=True)\n",
    "obj_fn = [config.pbias,config.nashsutcliffe,config.kge,\n",
    "          config.rmse,config.correlationcoefficient,config.mae,config.rrmse]\n",
    "obj_fn_names = ['PBIAS','NSE','KGE','RMSE','CORR','MAE','MAPE']\n",
    "error_columns = ['GaugeID','Latitude','Longitude','PBIAS','NSE','KGE','RMSE','CORR','MAE','RRMSE','obs_avg','sim_avg','MAPE','R2']\n",
    "error_df = pd.DataFrame(columns = error_columns).set_index('GaugeID')\n",
    "\n",
    "key = stations_list.loc[0,'stations']\n",
    "\n",
    "\n",
    "\n",
    "#Create list of all gauge stations with NaNs dropped and export to CSVs(if needed).\n",
    "stations = []\n",
    "for i in range(0,len(stations_list)):\n",
    "    key = stations_list.loc[i,'stations']\n",
    "    station = pd.read_csv('/home/civil/phd/cez218606/scratch/CLSM_Calibration/Narmada/final_run/output/uncal/'+key+'-merged.csv')\n",
    "    station.set_index('Date',inplace=True)\n",
    "    station.index = pd.to_datetime(station.index)\n",
    "    #station.dropna(inplace=True)\n",
    "    station = station.loc['2002-01-01':'2006-12-31'].dropna()\n",
    "    stations.append(station)\n",
    "\n",
    "new_row = [None]*len(stations)\n",
    "for i in range(0,len(stations)):\n",
    "    try:\n",
    "        obs_flow = stations[i]['Streamflow (cumecs)']\n",
    "        sim_flow = stations[i]['Streamflow_tavg']\n",
    "        meta_key = stations_list.loc[i,'stations']\n",
    "        s_info= {'lat':meta_file.at[meta_key,'Latitude'],'lon':meta_file.at[meta_key,'Longitude']}\n",
    "        error_columns = ['GaugeID','Latitude','Longitude','PBIAS','NSE','KGE_al','KGE','CORR','Alpha','Beta','RMSE','MAE','RRMSE','obs_avg','sim_avg','NMAE','R2','r_var']\n",
    "        error_df = pd.DataFrame(columns = error_columns).set_index('GaugeID')\n",
    "        new_row[i]= pd.DataFrame ([[meta_key,s_info['lat'],s_info['lon'],\n",
    "                                    round(config.pbias(obs_flow,sim_flow),3),\n",
    "                                    round(config.nashsutcliffe(obs_flow,sim_flow),3),\n",
    "                                    round(config.kge(obs_flow,sim_flow,return_all=\"True\")[0],3),\n",
    "                                    round(config.kge(obs_flow,sim_flow,return_all=\"True\")[1],3),\n",
    "                                    round(config.kge(obs_flow,sim_flow,return_all=\"True\")[2],3),\n",
    "                                    round(config.kge(obs_flow,sim_flow,return_all=\"True\")[3],3),\n",
    "                                    round(config.kge(obs_flow,sim_flow,return_all=\"True\")[4],3),\n",
    "                                    round(config.rmse(obs_flow,sim_flow),3),\n",
    "                                    round(config.mae(obs_flow,sim_flow),3),\n",
    "                                    round(config.rrmse(obs_flow,sim_flow),3),\n",
    "                                    round(obs_flow.mean(),3),\n",
    "                                    round(sim_flow.mean(),3),\n",
    "                                    round(config.nmae(obs_flow,sim_flow),3),\n",
    "                                    R2(obs_flow,sim_flow),\n",
    "                                    (1/((sim_flow-obs_flow).std())**2)]],\n",
    "                                    columns=(['GaugeID','Latitude','Longitude','PBIAS','NSE','KGE_al','KGE','CORR','Alpha','Beta','RMSE','MAE','RRMSE','obs_avg','sim_avg','NMAE','R2','r_var'])).set_index('GaugeID')\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        continue\n",
    "\n",
    "for i in range(0,len(new_row)):\n",
    "    error_df = error_df.append(new_row[i]) \n",
    "error_df['wf'] = (error_df['r_var'])/(error_df['r_var'].sum())\n",
    "error_df.to_csv('/home/civil/phd/cez218606/scratch/CLSM_Calibration/Narmada/final_run/output/uncal_error_matrix_2002-2006.csv')\n",
    "print(\"Python Run Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54de0300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Latitude       22.710733\n",
       "Longitude      78.183717\n",
       "PBIAS         -79.567750\n",
       "NSE             0.158500\n",
       "KGE_al         -0.974583\n",
       "KGE            -0.189333\n",
       "CORR            0.624417\n",
       "Alpha           0.212167\n",
       "Beta            0.204417\n",
       "RMSE         1013.317333\n",
       "MAE           310.708667\n",
       "RRMSE           3.335417\n",
       "obs_avg       400.309167\n",
       "sim_avg        91.275417\n",
       "NMAE            0.807500\n",
       "R2              0.410494\n",
       "r_var           0.000019\n",
       "wf              0.083333\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53b2e5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Latitude      22.710733\n",
       "Longitude     78.183717\n",
       "PBIAS        -48.599750\n",
       "NSE            0.475667\n",
       "KGE_al         0.025417\n",
       "KGE            0.317667\n",
       "CORR           0.708333\n",
       "Alpha          0.658417\n",
       "Beta           0.514000\n",
       "RMSE         716.342667\n",
       "MAE          251.360917\n",
       "RRMSE          2.646167\n",
       "obs_avg      400.309167\n",
       "sim_avg      223.816333\n",
       "NMAE           0.667083\n",
       "R2             0.526554\n",
       "r_var          0.000028\n",
       "wf             0.083333\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "968521ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Latitude      22.710733\n",
       "Longitude     78.183717\n",
       "PBIAS        -84.532917\n",
       "NSE            0.049250\n",
       "KGE_al        -1.147917\n",
       "KGE           -0.291833\n",
       "CORR           0.568667\n",
       "Alpha          0.142417\n",
       "Beta           0.154667\n",
       "RMSE         840.320917\n",
       "MAE          321.400500\n",
       "RRMSE          3.266167\n",
       "obs_avg      383.999583\n",
       "sim_avg       64.824250\n",
       "NMAE           0.858333\n",
       "R2             0.364545\n",
       "r_var          0.000018\n",
       "wf             0.083333\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b78c2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Latitude      22.710733\n",
       "Longitude     78.183717\n",
       "PBIAS        -57.839583\n",
       "NSE            0.314083\n",
       "KGE_al        -0.380417\n",
       "KGE            0.103917\n",
       "CORR           0.585917\n",
       "Alpha          0.476000\n",
       "Beta           0.421583\n",
       "RMSE         655.990917\n",
       "MAE          244.263750\n",
       "RRMSE          2.874500\n",
       "obs_avg      383.999583\n",
       "sim_avg      177.965250\n",
       "NMAE           0.691000\n",
       "R2             0.384068\n",
       "r_var          0.000026\n",
       "wf             0.083333\n",
       "dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c31b4b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Latitude      22.710733\n",
       "Longitude     78.183717\n",
       "PBIAS        -56.069667\n",
       "NSE            0.381500\n",
       "KGE_al        -0.202583\n",
       "KGE            0.187083\n",
       "CORR           0.635833\n",
       "Alpha          0.558417\n",
       "Beta           0.439167\n",
       "RMSE         616.639833\n",
       "MAE          224.346917\n",
       "RRMSE          2.941000\n",
       "obs_avg      334.871167\n",
       "sim_avg      162.251417\n",
       "NMAE           0.704333\n",
       "R2             0.429144\n",
       "r_var          0.000027\n",
       "wf             0.083333\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6c515bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Latitude      22.710733\n",
       "Longitude     78.183717\n",
       "PBIAS        -82.799000\n",
       "NSE            0.105000\n",
       "KGE_al        -1.058750\n",
       "KGE           -0.246083\n",
       "CORR           0.577000\n",
       "Alpha          0.180667\n",
       "Beta           0.172083\n",
       "RMSE         800.291333\n",
       "MAE          275.789167\n",
       "RRMSE          3.443250\n",
       "obs_avg      334.871167\n",
       "sim_avg       63.790000\n",
       "NMAE           0.844833\n",
       "R2             0.353249\n",
       "r_var          0.000019\n",
       "wf             0.083333\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ade8ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
